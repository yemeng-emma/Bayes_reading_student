---
title: "Week2"
author: "Meng Ye"
date: "2022-08-31"
output: html_document
---

```{r load packages, message=FALSE, warning=FALSE}
library(bayesrules)
library(tidyverse)
library(janitor)
```

# Bayes Rules Chapter 2

## Follow along Bayes Rules Chapter 2

### Intro

```{r}
# Import article data
data(fake_news)
```

ps: data takes some to load...


```{r}
fake_news %>% 
  tabyl(type) %>% 
  adorn_totals("row")
```

ps: get to know a new way to derive the "pretend" row of summed up data, but maybe compared with our earlier `add_row()` approach, the types of calculation for the summed up row might be limited to only `sum()`.

```{r}
# Tabulate exclamation usage and article type
fake_news %>% 
  tabyl(title_has_excl, type) %>% 
  adorn_totals("row")
```



### 2.1 Building a Beyesian model for the events 

- Conditional vs. unconditional probability

- Definition of independent events:

  if and only if $P(A|B) = P(A)$
  
- Probability vs. Likelihood 
  Notice that the prior probabilities add up to 1 but the likelihoods do not. Again, the likelihood function is not a probability function, but rather provides a framework to compare the relative compatibility of our aclamation point data with $B$ and $B^c$.
  
  likelihood function defined as:
  
  $L(B|A) = P(A|B)$ and $L(B^c|A) = P(A|B^c)$
  
- Marginal probability and the LTP 

```{r}
# try out the calculation by adapting author's code
fake_news %>% 
  tabyl(title_has_excl, type) %>% 
  adorn_totals("row") %>% 
  adorn_totals("col") %>% 
  mutate(across(where(is.numeric), ~ ./150)) %>% 
  adorn_rounding(digits = 4)
```


This is calculation in Table 2.3.

#### 2.1.5 Posterier simulation 
```{r}
# Define possible articles
article <- data.frame(type = c("real", "fake"))

# Define the prior model
prior <- c(0.6, 0.4)

# Simulate 3 articles
set.seed(84735)
sample_n(article, size = 3, weight = prior, replace = TRUE)
```
ps. Have some trivil questions:

How does R know to correspond the order of `article` and `prior` 
`replace = TRUE` is R code way to say "put the ball drawn back to the bag"? and `replace = FALSE` is not puting the ball back? 


```{r}
# Simulate 10000 articles. 
set.seed(84735)

article_sim <- sample_n(article, size = 10000, 
                        weight = prior, replace = TRUE)
# skip the (ugly) bar chart

# tab of the 10000 sample
article_sim %>% 
  tabyl(type) %>% 
  adorn_totals("row")

```


```{r}
# simulate the exclamation points
article_sim <- article_sim %>% 
  mutate(data_model = case_when(type == "fake" ~ 0.2667,
                                type == "real" ~ 0.0222))

glimpse(article_sim)
```

```{r}
# Define whether there are exclamation points
data <- c("no", "yes")

# Simulate exclamation point usage 
set.seed(3)
article_sim <- article_sim %>%
  group_by(1:n()) %>% 
  mutate(usage = sample(data, size = 1, 
                        prob = c(1 - data_model, data_model)))
```

ps: in these simulations what we feed first to `sample()` or `sample_n()` is the "sample space"? 

"Note that `sample()` is similar to `sample_n()` but samples values from vectors instead of rows from data frames."

`sample()` uses "prob", and `sample_n()` uses "weights". 


```{r}
article_sim %>% 
  tabyl(usage, type) %>% 
  adorn_totals(c("col","row"))
```

ps: ok, learned one more trick, you can do row and col together with `c()`

```{r}
# adapted bar chart
ggplot(article_sim, aes(x = type, fill = type)) + 
  geom_bar(width = 0.6) + 
  facet_wrap(~ fct_rev(usage)) +
  theme_bw()
```

### 2.2 Example: pop vs soda vs coke

```{r}
# Load the data
data(pop_vs_soda)

# Summarize pop use by region
pop_vs_soda %>% 
  tabyl(pop, region) %>% 
  adorn_percentages("col")  %>% 
  adorn_rounding(digits = 4)
```



### 2.3 Building a Bayesian model for random variables 

pivoting from categorical variables to **numerical** variables 

- the Binomial model 

notation:

$Y|\pi \sim Bin (n, \pi)$

ps. the meaning of "parameters"?
In my earlier stats training "parameters" are the true value of the estimated coefficients in the "population". Here it seems to mean something different, my understanding is that: parameters means sth set, what is not "random variables"


- normalizing constant:
  can be treated just as a constant $c$
  
- posterior shortcut:

  $posterior \propto prior \cdot likelihood$
  
- posterior simulation
  
```{r}
# Define possible win probabilities
chess <- data.frame(pi = c(0.2, 0.5, 0.8))

# Define the prior model
prior <- c(0.10, 0.25, 0.65)

# Simulate 10000 values of pi from the prior
set.seed(84735)
chess_sim <- sample_n(chess, size = 10000, weight = prior, replace = TRUE)

# Simulate 10000 match outcomes
chess_sim <- chess_sim %>% 
  mutate(y = rbinom(10000, size = 6, prob = pi))

# Check it out
chess_sim %>% 
  head(3)
```

```{r}
# Summarize the prior
chess_sim %>% 
  tabyl(pi) %>% 
  adorn_totals("row")
```

```{r}
# Plot y by pi
ggplot(chess_sim, aes(x = y)) + 
  stat_count(aes(y = ..prop..)) + 
  facet_wrap(~ pi)
```

```{r}
# Focus on simulations with y = 1
win_one <- chess_sim %>% 
  filter(y == 1)

# Summarize the posterior approximation
win_one %>% 
  tabyl(pi) %>% 
  adorn_totals("row")


# Plot the posterior approximation
ggplot(win_one, aes(x = pi)) + 
  geom_bar()
```

## Questions for the chapter

1. Needs more explanation of the definition of marginal probability, or better -- marginal prob. vs. conditional prob. vs. combined prob.


2. In a real research question, which corresponds to prior, which corresponds to postierior?

3. How Bayesian model is different from MLE? 

4. Does the prior and posterior elements of Bayesian models necessarily indicate there should be temporal dimension to the data analyzed?  



## Practices for Beyes Rules Chapter 2

### Excercise 2.2 

Save for later. Maybe we go through it together?

### Excercise 2.4

$ P(V) = 0.05$ , $P(V^c) = 0.95$
$ P(S|V) = 0.7$, $P(S|V^c) = 0.03$ 

$$
P(V|S) = \frac{P(S|V) \cdot P(V)}{P(S|V) \cdot P(V) + P(S|V^c) \cdot P(V^c)} = 0.7*0.05/(0.7*0.05 + 0.03*0.95) = 0.55
$$

```{r}
0.7*0.05/(0.7*0.05 + 0.03*0.95)
```


ps: How do I start a new line without having to type so much "$"

### Excercise 2.14 

to do later

### Excercise 2.20

to do later

```{r}

```



# Bayes Rules Chapter 3

## Follow along Bayes Rules Chapter 3

### Intro

- Beta-Binomial Bayesian model

a continuous prior probability model of $\pi$, provides us the tools we need to study the proportion of interest,

### 3.1 Beta prior model 

- pdf ~ pmf for discrete variables and integrates to 1, probability is the area

- A hyperparameter is a parameter used in a prior model

- Question for Equation 3.1, what is the "y" in the part $e^{-y}$

_ And just need a little explanation on the "argmax" notation 

- What scenarios generally have a $\pi$ distribution of the Beta model? 

- Tuning the Beta prior 

ps. meaning what is our best guess of alpha and beta given what we know in the piror data?

how to tune:

1) find the mean/expectation = 0.45

and make sure $E(\pi) = \alpha/(\alpha + \beta) \sim 0.45$ holds 

ps. that gives us the ratio between the two parameters, are there other clues for these two should sum up?

```{r}
# Plot the Beta(45, 55) prior
plot_beta(45, 55)
```


### 3.2 The Binomial data model and likelihood function 

ps. that is the second element of the Bayesian Rules

- turning the head

  The likelihood function is knowing the result y = 30 as the constant and is a function of the prob $\pi$ 
  
- maximization 

  Author didn't explain how they found that pi = 0.6 maximize the likelihood function. But I know R can do it for us. And this really resembles *MLE*!
  
  
## 3.3 Beta posterior model 

The third/last element of the Bayes Rules and it is what we try to find the answer for. And it is a model of which variable? $\pi$!
How we re-estimate the prob distribution of some events after we update our knowledge about new data.

The ONGO data, the ROs get registered before the law and after law, OR registered in Year 1 and Year 2 and Year 3?

```{r}
plot_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)
```

ps: neat plot! really intuitive to help you get the rationale


```{r}
summarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)
```

- How the posterior is built?

Function 3.8 is genius! And that's all we need to build the posterior as the case in the chunk above, alpha, beta, y and n. 

Q: what we quickly go through the meaning of kernel, certainly it is a general concept. We saw it in RD. But what it means exactly? 

### 3.4 Beta-Benomial model

- general form

$$ Y| \sim Bin (n, \pi)
$$
$$\pi \sim Beta(\alpha, beta)
$$

- posterior of pi


$$ \pi| (Y = y) \sim Beta (\alpha + y, \beta + n-y)
$$ 




### 3.5 Simulation 

```{r}
set.seed(84735)
michelle_sim <- data.frame(pi = rbeta(10000, 45, 55)) %>% 
  mutate(y = rbinom(10000, size = 50, prob = pi))

ggplot(michelle_sim, aes(x = pi, y = y)) + 
  geom_point(aes(color = (y == 30)), size = 0.1)
```

```{r}
# Keep only the simulated pairs that match our data
michelle_posterior <- michelle_sim %>% 
  filter(y == 30)

# Plot the remaining pi values
ggplot(michelle_posterior, aes(x = pi)) + 
  geom_density()
```


```{r}
michelle_posterior %>% 
  summarize(mean(pi), sd(pi))
  mean(pi)  sd(pi)
```


```{r}
nrow(michelle_posterior)
```


## Questions for the chapter

1. In what circumstances do we model pi with the Beta probability model?

2. Why define the likelihood function? The conditional prob is more intuitive to me.



## Practices for Beyes Rules Chapter 3




# Stats Rethinking Chapter 2

## Follow along Bayes Rules Chapter 2

## Questions for the chapter

## Practices for Beyes Rules Chapter 2






# Stats Rethinking Chapter 3

## Follow along Bayes Rules Chapter 3

## Questions for the chapter

## Practices for Beyes Rules Chapter 3













